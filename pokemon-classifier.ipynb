{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saqeeb's Pokedex\n",
    "\n",
    "\n",
    "Welcome to my own personal Pokedex, where I'll be using a residual neural network to build a classifier that can identify any of the original 151 pokemon!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to grab the data. Luckily there is already a nice dataset on kaggle.com.\n",
    "It is, however, missing two pokemon: the male and female nidorans. I'm planning to gather the data for this later. Right now we'll be training with 149 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pokemon-generation-one.zip to .\\pokemon-generation-one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.15G/2.15G [00:57<00:00, 40.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import cv2\n",
    "import opendatasets as od\n",
    "\n",
    "dataset_url = 'https://www.kaggle.com/thedagger/pokemon-generation-one' # You will need a kaggle token for this\n",
    "od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abra', 'Aerodactyl', 'Alakazam', 'Arbok', 'Arcanine', 'Articuno', 'Beedrill', 'Bellsprout', 'Blastoise', 'Bulbasaur', 'Butterfree', 'Caterpie', 'Chansey', 'Charizard', 'Charmander', 'Charmeleon', 'Clefable', 'Clefairy', 'Cloyster', 'Cubone', 'Dewgong', 'Diglett', 'Ditto', 'Dodrio', 'Doduo', 'Dragonair', 'Dragonite', 'Dratini', 'Drowzee', 'Dugtrio', 'Eevee', 'Ekans', 'Electabuzz', 'Electrode', 'Exeggcute', 'Exeggutor', 'Farfetchd', 'Fearow', 'Flareon', 'Gastly', 'Gengar', 'Geodude', 'Gloom', 'Golbat', 'Goldeen', 'Golduck', 'Golem', 'Graveler', 'Grimer', 'Growlithe', 'Gyarados', 'Haunter', 'Hitmonchan', 'Hitmonlee', 'Horsea', 'Hypno', 'Ivysaur', 'Jigglypuff', 'Jolteon', 'Jynx', 'Kabuto', 'Kabutops', 'Kadabra', 'Kakuna', 'Kangaskhan', 'Kingler', 'Koffing', 'Krabby', 'Lapras', 'Lickitung', 'Machamp', 'Machoke', 'Machop', 'Magikarp', 'Magmar', 'Magnemite', 'Magneton', 'Mankey', 'Marowak', 'Meowth', 'Metapod', 'Mew', 'Mewtwo', 'Moltres', 'MrMime', 'Muk', 'Nidoking', 'Nidoqueen', 'Nidorina', 'Nidorino', 'Ninetales', 'Oddish', 'Omanyte', 'Omastar', 'Onix', 'Paras', 'Parasect', 'Persian', 'Pidgeot', 'Pidgeotto', 'Pidgey', 'Pikachu', 'Pinsir', 'Poliwag', 'Poliwhirl', 'Poliwrath', 'Ponyta', 'Porygon', 'Primeape', 'Psyduck', 'Raichu', 'Rapidash', 'Raticate', 'Rattata', 'Rhydon', 'Rhyhorn', 'Sandshrew', 'Sandslash', 'Scyther', 'Seadra', 'Seaking', 'Seel', 'Shellder', 'Slowbro', 'Slowpoke', 'Snorlax', 'Spearow', 'Squirtle', 'Starmie', 'Staryu', 'Tangela', 'Tauros', 'Tentacool', 'Tentacruel', 'Vaporeon', 'Venomoth', 'Venonat', 'Venusaur', 'Victreebel', 'Vileplume', 'Voltorb', 'Vulpix', 'Wartortle', 'Weedle', 'Weepinbell', 'Weezing', 'Wigglytuff', 'Zapdos', 'Zubat']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = './pokemon-generation-one/dataset'\n",
    "classes = os.listdir(DATA_DIR)\n",
    "print(classes)\n",
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the images, we first have to calculate the mean and standard deviation of the dataset. This takes a while and is bottlenecked by my HDD, so we'll try to read it from a file, and if the file doesn't exist, we'll calculate it manually and save it to a file for next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats.data does not exist in the root directory. Calculating the mean and standard deviation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:976: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "\n",
    "def mean_std(batch):\n",
    "    '''Calculate the means and standard deviations of all images in a batch by colour channel'''\n",
    "    images, labels = next(iter(batch))\n",
    "    # shape of images = [b,c,w,h]\n",
    "    mean, std = images.mean([0,2,3]), images.std([0,2,3])\n",
    "    return mean, std\n",
    "\n",
    "try:\n",
    "    with open('stats.data', 'r') as filehandle: # open file for reading\n",
    "        stats = json.load(filehandle)\n",
    "    print(\"Loaded mean and standard deviation from file.\")\n",
    "    print(stats)\n",
    "    mean, std = stats\n",
    "    write = False\n",
    "except:\n",
    "    print(\"stats.data does not exist in the root directory. Calculating the mean and standard deviation:\")\n",
    "    write = True\n",
    "\n",
    "if write:\n",
    "    # Create a temporary dataset to get the mean and std\n",
    "\n",
    "    image_size = 128\n",
    "\n",
    "    stats_tfms = tt.Compose([tt.Resize(image_size),\n",
    "                            tt.CenterCrop(image_size),\n",
    "                            tt.ToTensor()])\n",
    "\n",
    "    stats_set = ImageFolder(DATA_DIR, stats_tfms)\n",
    "    dl = DataLoader(stats_set, batch_size=len(stats_set))\n",
    "\n",
    "    mean, std = mean_std(dl)\n",
    "    stats = mean.tolist(), std.tolist()\n",
    "    print(stats)\n",
    "\n",
    "    with open('stats.data', 'w') as filehandle: # open file for writing\n",
    "        json.dump((stats), filehandle)\n",
    "    print(\"Stats saved in stats.data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to standardize the dataset in terms of image size. Due to memory constraints, each image should be no more than 128x128 pixels. Below, I create two instances of the dataset. That's because I don't have the files separated into training and validations sets, so instead I applied two random_split()s with the same seed. This allows for the training and validation sets to have different transformations/augmentations applied to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "batch_size = 64\n",
    "stats = mean, std\n",
    "\n",
    "train_tfms = tt.Compose([tt.Resize(image_size),\n",
    "                         # tt.Resize(image_size, max_size=image_size),\n",
    "                         # tt.Pad(padding=[(128 - image[0])/2 if image[0]<128 else 0, (128 - image[1])/2 if image[1]<128 else 0]),\n",
    "                         tt.CenterCrop(image_size),\n",
    "                         tt.RandomCrop(image_size, padding=8, padding_mode='reflect'), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         # tt.RandomRotate\n",
    "                         # tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n",
    "                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1), \n",
    "                         tt.ToTensor(),\n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "\n",
    "valid_tfms = tt.Compose([tt.Resize(image_size),\n",
    "                         tt.CenterCrop(image_size),\n",
    "                         tt.ToTensor(),\n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "\n",
    "dataset  = ImageFolder(DATA_DIR, train_tfms)\n",
    "dataset2 = ImageFolder(DATA_DIR, valid_tfms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset into training and validation sets, with a seed for consistency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);\n",
    "\n",
    "valid_percent = 0.2\n",
    "valid_size = int(valid_percent * len(dataset))\n",
    "train_size = len(dataset) - valid_size\n",
    "\n",
    "train_ds,  valid_ds  = random_split(dataset,  [train_size, valid_size]) # has the train_tfms transforms\n",
    "\n",
    "torch.manual_seed(random_seed);\n",
    "train_ds2, valid_ds2 = random_split(dataset2, [train_size, valid_size]) # has the valid_tfms transforms\n",
    "\n",
    "del train_ds2\n",
    "del valid_ds\n",
    "\n",
    "len(train_ds), len(valid_ds2), len(dataset) # Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to be able to do some analytics and figure out the best way to tune our model. Let's record the number of examples in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = False\n",
    "if 'total_examples_per_class' not in locals(): count = True\n",
    "\n",
    "\n",
    "def count_examples(train_dataset, valid_dataset):\n",
    "    train_examples_per_class = [0] * num_classes\n",
    "    valid_examples_per_class = [0] * num_classes\n",
    "    total_examples_per_class = [0] * num_classes\n",
    "\n",
    "    for _, label in train_dataset:\n",
    "        train_examples_per_class[label] += 1\n",
    "        total_examples_per_class[label] += 1\n",
    "\n",
    "    for _, label in valid_dataset:\n",
    "        valid_examples_per_class[label] += 1\n",
    "        total_examples_per_class[label] += 1\n",
    "    return train_examples_per_class, valid_examples_per_class, total_examples_per_class\n",
    "\n",
    "if count:\n",
    "    train_per_class, valid_per_class, total_per_class = count_examples(train_ds, valid_ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up some data loaders. I'll start with a batch size of 64, and increase it if it's training too slowly and I have extra GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds2, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the images to make sure nothing went wrong so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def denormalize(images, means, stds):\n",
    "    means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
    "    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
    "    return images * stds + means\n",
    "    # return (images-means)/stds\n",
    "\n",
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        denorm_images = denormalize(images, *stats)\n",
    "        ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dl: # Sanity check\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data onto the GPU\n",
    "\n",
    "Alright, we've got all our data formatted and ready to go! Let's load everything onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a wrapper `DeviceDataLoader` to wrap around our dataloaders and transfer them to the GPU. I think this code came from fast.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "print(device)\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "We'll start with a 9 layer ResNet as described [here](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet/). The layers are shown in this image:\n",
    "\n",
    "![resnet-9](https://github.com/lambdal/cifar10-fast/raw/master/net.svg?sanitize=true)\n",
    "\n",
    "First, let's define a base classification class that has useful methods for reporting the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassification(nn.Module):\n",
    "    # The constructor will go in the child class\n",
    "    def training_loss(self, batch):\n",
    "        images, labels = batch\n",
    "        predictions = self(images)                  # get predictions\n",
    "        loss = F.cross_entropy(predictions, labels) # calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_loss(self, batch):\n",
    "        images, labels = batch\n",
    "        predictions = self(images)                  # get predictions\n",
    "        loss = F.cross_entropy(predictions, labels) # calculate loss\n",
    "        acc = accuracy(predictions, labels)         # calculate accuracy\n",
    "        return {\"val_loss\": loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_stats(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        batch_accs   = [x['val_acc']  for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        epoch_acc  = torch.stack(batch_accs).mean()     # Combine accuracies\n",
    "        return {'val_loss_epoch': epoch_loss.item(), 'val_acc_epoch': epoch_acc.item()}\n",
    "\n",
    "    def report_epoch_performance(self, epoch, result):\n",
    "        print(\"Epoch {}, learning rate: {:.5f}, trainLoss: {:.4f}, valLoss: {:.4f}, valAcc: {:.4f}\".format(\n",
    "            epoch+1, result['lrs'][-1], result['train_loss'], result['val_loss_epoch'], result['val_acc_epoch']))\n",
    "\n",
    "# Need to define the accuracy function called above\n",
    "def accuracy(predictions, labels):\n",
    "    # input 'predictions' is an array of batch_index * class_probabilities\n",
    "    _, predictions = torch.max(predictions, dim=1)  # get index of max probability\n",
    "    acc = torch.sum(predictions == labels).item() / len(predictions)\n",
    "    return torch.tensor(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we expand this class with the model architecture. The ResNet9 architecture follows up each convolution with a batch normalization and ReLU, and sometimes a max pool. Let's define a function that simplifies this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "              nn.BatchNorm2d(out_channels),\n",
    "              nn. ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a ResNet9 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet9(ImageClassification):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "\n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "\n",
    "        self.conv5 = conv_block(512, 512, pool=True)\n",
    "        self.conv6 = conv_block(512, 512, pool=True)\n",
    "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "\n",
    "        self.classify = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                      # nn.MaxPool2d(2),\n",
    "                                      # nn.MaxPool2d(2),\n",
    "                                      nn.Flatten(),\n",
    "                                      nn.Dropout(0.2),\n",
    "                                      nn.Linear(512, num_classes),\n",
    "                                      nn.Softmax(dim=1)\n",
    "                                      )\n",
    "        \n",
    "    def forward(self, image_batch):\n",
    "        out = self.conv1(image_batch)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out # Residual Layer\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "\n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.res3(out) + out\n",
    "\n",
    "        out = self.classify(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(ResNet9(3, num_classes), device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we've defined our model. Next we have to set up our training loop. We'll be training with learning rate scheduling, weight decay, and gradient clipping. Hopefully this will get us to a good loss minimum without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, valid_loader, num_epochs, max_lr, \n",
    "        weight_decay=0, grad_clip=None, opt_func = torch.optim.SGD, stop_pct=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "\n",
    "    # Set up optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Set up the learning rate scheduler (one-cycle learning rate)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=num_epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        # Training step\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_loss(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping\n",
    "            if grad_clip:\n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Record and update learning rate\n",
    "            lrs.append(get_lr(optimizer))       # Need to define this function\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation Phase - report performance stats for each epoch\n",
    "        result = evaluate(model, valid_loader)  # Need to define this function\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.report_epoch_performance(epoch, result)\n",
    "        history.append(result)\n",
    "        if stop_pct:\n",
    "            if history[-1]['val_acc_epoch'] >= stop_pct:\n",
    "                print(\"Validation accuracy passed 97%, stopping training.\")\n",
    "                break\n",
    "    return history\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:  # I don't understand how this works\n",
    "        return param_group['lr']\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, valid_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_loss(batch) for batch in valid_loader]\n",
    "    return model.validation_epoch_stats(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cache = True\n",
    "\n",
    "if clear_cache: torch.cuda.empty_cache()\n",
    "\n",
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the model! We'll use the adam optimizer along with the following hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "max_lr = 0.001\n",
    "grad_clip = 0.01\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam\n",
    "stop_pct = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history += fit(model, train_dl, valid_dl, num_epochs, max_lr, weight_decay, grad_clip, opt_func, stop_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to save my hyperparameters for training runs that had good results. We can save this to a file, and in the future we can look at this file in its git commit and grab the values. We can also save the model parameters using a pytorch `state_dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "load = True\n",
    "\n",
    "filename = 'f'\n",
    "\n",
    "if save:\n",
    "    hyperparams = {'num_epochs': num_epochs,\n",
    "                'max_lr': max_lr,\n",
    "                'grad_clip': grad_clip,\n",
    "                'weight_decay': weight_decay,\n",
    "                'opt_func': torch.optim.Adam,\n",
    "                'history': history,\n",
    "                'model': model.state_dict(),\n",
    "                'val_accuracy': history[-1]['val_acc_epoch'],\n",
    "                'train_loss': history[-1]['train_loss'],\n",
    "                'val_loss': history[-1]['val_loss_epoch']\n",
    "                }\n",
    "\n",
    "    torch.save(hyperparams, filename+'.pth')\n",
    "\n",
    "if load:\n",
    "    hyperparams = torch.load(filename+'.pth')\n",
    "    history = hyperparams['history']\n",
    "    model = to_device(ResNet9(3, num_classes), device)\n",
    "    model.load_state_dict(hyperparams['model'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the accuracy and losses of the model over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_acc_epoch'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss_epoch'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analytics\n",
    "\n",
    "We also want to know things like:\n",
    "* The accuracy of predicting each class vs the number of training examples in that class\n",
    "* Which classes have the most false positives\n",
    "* Which classes have the poorest performance\n",
    "* Which classes get mistaken by which other classes most often\n",
    "\n",
    "Most of these can be calculated from a confusion matrix, so let's create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "\n",
    "    # Pick index with highest probability\n",
    "    # _, preds  = torch.max(yb, dim=1)\n",
    "\n",
    "    # Sort predictions to get most likely predictions\n",
    "    probs, pred = torch.sort(yb, dim=1, descending=True)\n",
    "    \n",
    "    return probs[0].tolist(), pred[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model, valid_dataset, num_classes):\n",
    "    matrix = to_device(torch.zeros(num_classes, num_classes), device)\n",
    "    # dimensions will be true labels * predictions\n",
    "    for image, label in valid_dataset:\n",
    "        _, predictions = predict_image(image, model)\n",
    "        matrix[label][predictions[0]] += 1\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(model, valid_ds2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some functions for plotting analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# matplotlib.style.use('ggplot') # This was a mistake\n",
    "\n",
    "# To solve the plt.ylabel \"str object is not callable\" error\n",
    "# from importlib import reload\n",
    "# reload(plt)\n",
    "\n",
    "def accuracy_per_class(confusion_matrix):\n",
    "    totals = torch.sum(confusion_matrix, dim=1)\n",
    "    return torch.div(torch.diag(confusion_matrix), totals).tolist()\n",
    "\n",
    "def plot_accuracy_vs_examples(acc_per_class, train_per_class):\n",
    "    plt.plot(train_per_class, acc_per_class, 'bx')\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([0, max(train_per_class)+10])\n",
    "    plt.xlabel(\"Number of training examples\")\n",
    "    plt.ylabel(\"Accuracy on validation set\")\n",
    "    plt.grid()\n",
    "\n",
    "def plot_acc_vs_class(acc_per_class, classes, sorted=False):\n",
    "    df = pd.DataFrame({'acc_per_class': acc_per_class, 'classes': classes})\n",
    "    if sorted:\n",
    "        df.sort_values('acc_per_class', inplace=True)\n",
    "    df.plot(kind='bar', x='classes', legend=False, title='Class Accuracies', rot=0)\n",
    "    plt.ylabel('Accuracies')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_class = accuracy_per_class(matrix)\n",
    "plot_accuracy_vs_examples(acc_per_class, train_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_vs_class(acc_per_class, classes, sorted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with individual images\n",
    "\n",
    "Let's see what the model predicts for some hand-picked examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "index = [x for x in range(100)]\n",
    "shuffle(index)\n",
    "index = iter(index)\n",
    "# index = shuffle(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = valid_ds2[next(index)]\n",
    "plt.imshow(denormalize(img.unsqueeze(0), *stats).squeeze().permute(1, 2, 0).clamp(0, 1))\n",
    "\n",
    "probs, preds = predict_image(img, model)\n",
    "top_preds = [classes[x] for x in preds[0:5]]\n",
    "percents = [100*prob for prob in probs]\n",
    "\n",
    "print(\"Label:\", classes[label], '\\n')\n",
    "str = [\"Guess: {:11} Probability: {:.3f}\".format(top_preds[i], percents[i]) for i in range(len(top_preds))]\n",
    "for line in str:\n",
    "    print(line)\n",
    "\n",
    "# print('Label:', classes[label], ', Predicted:', predict_image(img, model))\n",
    "# print('Label:', classes[label], ', Predicted:', top_preds, ', Probabilities:', percents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "haha! works 10% of the time, every time\n",
    "\n",
    "\n",
    "Let's test on images found online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from importlib import reload\n",
    "reload(plt)\n",
    "\n",
    "if 'img' in locals(): del img\n",
    "\n",
    "# url = \"https://static.wikia.nocookie.net/sonicpokemon/images/1/17/Quagsire.jpg/revision/latest/scale-to-width-down/640?cb=20130626042020\"\n",
    "\n",
    "# Get the image\n",
    "try:\n",
    "    url = input(\"Enter the image url: \")\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.convert('RGB')\n",
    "except:\n",
    "    print(\"There was a problem obtaining the image.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Convert to what the model expects\n",
    "img = valid_tfms(img)\n",
    "\n",
    "# Show the image\n",
    "denorm_image = denormalize(img, *stats).squeeze()\n",
    "plt.imshow(denorm_image.permute([1, 2, 0]).clamp(0, 1))\n",
    "plt.grid(None)\n",
    "\n",
    "# Get the prediction\n",
    "probs, preds = predict_image(img, model)\n",
    "top_preds = [classes[x] for x in preds[0:5]]\n",
    "percents = [100*prob for prob in probs]\n",
    "\n",
    "# Display the prediction\n",
    "str = [\"Guess: {}, Probability: {}\".format(top_preds[i], percents[i]) for i in range(len(top_preds))]\n",
    "for line in str:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
